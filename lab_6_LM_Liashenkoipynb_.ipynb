{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OppnFN4KUJj",
        "outputId": "4c059ded-2568-4391-b1a3-02695646857b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Завантаження корпусу (приклад для Brown)\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.corpus.conll2000\n",
        "nltk.download('conll2000')\n",
        "\n",
        "from nltk.corpus import conll2000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMaBF49ZKWiD",
        "outputId": "e220ee04-61bb-44a8-fb9a-1d00c0b0fb04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sents = list(conll2000.tagged_sents())"
      ],
      "metadata": {
        "id": "-_04cqz_Lpz5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Перемішування та розділення на навчальну і тестову вибірки\n",
        "random.seed(42)\n",
        "random.shuffle(tagged_sents)\n",
        "\n",
        "split_point = int(len(tagged_sents) * 0.8)\n",
        "train_sents = tagged_sents[:split_point]\n",
        "test_sents = tagged_sents[split_point:]\n",
        "\n",
        "# Запис у файли\n",
        "def write_tagged_sentences(sentences, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            for word, tag in sentence:\n",
        "                f.write(f\"{word}\\t{tag}\\n\")\n",
        "            f.write(\"\\n\")  # Порожній рядок між реченнями\n",
        "\n",
        "write_tagged_sentences(train_sents, \"conll2000_training.pos\")\n",
        "write_tagged_sentences(test_sents, \"conll2000_test.pos\")\n",
        "\n",
        "# Створення словника - виправлена версія\n",
        "word_counter = Counter()\n",
        "for sentence in train_sents:\n",
        "    for word, _ in sentence:\n",
        "        word_counter[word] += 1\n",
        "\n",
        "# Збереження слів, які з'являються принаймні двічі\n",
        "vocab = {word for word, count in word_counter.items() if count >= 2}\n",
        "\n",
        "with open(\"conll2000_vocab.txt\", 'w', encoding='utf-8') as f:\n",
        "    for word in sorted(vocab):\n",
        "        f.write(f\"{word}\\n\")\n",
        "\n",
        "# Створення файлу тестових слів\n",
        "with open(\"conll2000_test_words.txt\", 'w', encoding='utf-8') as f:\n",
        "    for sentence in test_sents:\n",
        "        for word, _ in sentence:\n",
        "            f.write(f\"{word}\\n\")\n",
        "        f.write(\"\\n\")  # Порожній рядок між реченнями"
      ],
      "metadata": {
        "id": "zTqo4amyMGdz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Символи пунктуації\n",
        "punct = set(string.punctuation)\n",
        "\n",
        "# Морфологічні правила, що використовуються для класифікації невідомих слів\n",
        "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n"
      ],
      "metadata": {
        "id": "mLtR-wb1NH2-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_unk(tok):\n",
        "    \"\"\"\n",
        "    Призначення міток для невідомих слів\n",
        "    \"\"\"\n",
        "    # Цифри\n",
        "    if any(char.isdigit() for char in tok):\n",
        "        return \"--unk_digit--\"\n",
        "\n",
        "    # Пунктуація\n",
        "    elif any(char in punct for char in tok):\n",
        "        return \"--unk_punct--\"\n",
        "\n",
        "    # Великі літери\n",
        "    elif any(char.isupper() for char in tok):\n",
        "        return \"--unk_upper--\"\n",
        "\n",
        "    # Іменники\n",
        "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"--unk_noun--\"\n",
        "\n",
        "    # Дієслова\n",
        "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"--unk_verb--\"\n",
        "\n",
        "    # Прикметники\n",
        "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"--unk_adj--\"\n",
        "\n",
        "    # Прислівники\n",
        "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"--unk_adv--\"\n",
        "\n",
        "    return \"--unk--\""
      ],
      "metadata": {
        "id": "vWFcK-ctOFk4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(vocab, data_fp):\n",
        "    \"\"\"\n",
        "    Попередня обробка даних\n",
        "    \"\"\"\n",
        "    orig = []\n",
        "    prep = []\n",
        "\n",
        "    # Додаю спеціальний токен для кінця речення\n",
        "    if \"--n--\" not in vocab:\n",
        "        vocab[\"--n--\"] = len(vocab)\n",
        "\n",
        "    # Читання даних\n",
        "    with open(data_fp, \"r\") as data_file:\n",
        "        for cnt, word in enumerate(data_file):\n",
        "            # Кінець речення\n",
        "            if not word.split():\n",
        "                orig.append(word.strip())\n",
        "                word = \"--n--\"\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            # Обробка невідомих слів\n",
        "            elif word.strip() not in vocab:\n",
        "                orig.append(word.strip())\n",
        "                word = assign_unk(word.strip())  # Виправлення: додано strip()\n",
        "                prep.append(word)\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                orig.append(word.strip())\n",
        "                prep.append(word.strip())\n",
        "\n",
        "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
        "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
        "\n",
        "    return orig, prep\n"
      ],
      "metadata": {
        "id": "h3j4b3ueOkPF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Завантаження навчального корпусу\n",
        "with open(\"/content/conll2000_training.pos\", 'r') as f:\n",
        "    training_corpus = f.readlines()\n",
        "\n",
        "# Завантаження словника\n",
        "with open(\"/content/conll2000_vocab.txt\", 'r') as f:\n",
        "    voc_l = f.read().split('\\n')\n",
        "\n",
        "# Створення словника з індексами для слів\n",
        "vocab = {}\n",
        "for i, word in enumerate(sorted(voc_l)):\n",
        "    vocab[word] = i\n",
        "\n",
        "# Завантаження тестового корпусу\n",
        "with open(\"/content/conll2000_test.pos\", 'r') as f:\n",
        "    y = f.readlines()\n",
        "\n",
        "# Попередня обробка тестових слів\n",
        "_, prep = preprocess(vocab, \"/content/conll2000_test_words.txt\")\n"
      ],
      "metadata": {
        "id": "sVn2-VjnO4SE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ciRa5TtvPRRz",
        "outputId": "d25d0fff-aab6-457c-b90a-ff953874e788"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'No\\tDT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "WSerG9KRPqM8",
        "outputId": "2d2c47fb-987f-4f26-cd00-6397308fb345"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'But\\tCC\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Виправлення 2: Виправлена функція get_word_tag\n",
        "def get_word_tag(line, vocab):\n",
        "    \"\"\"\n",
        "    Отримання слова та його тегу з рядка корпусу.\n",
        "    \"\"\"\n",
        "    if not line.split():\n",
        "        word = \"--n--\"\n",
        "        tag = \"--s--\"\n",
        "        return word, tag  # Виправлення: додано повернення значення\n",
        "    else:\n",
        "        parts = line.split()\n",
        "        if len(parts) >= 2:\n",
        "            word, tag = parts[0], parts[1]\n",
        "            if word not in vocab:\n",
        "                # Обробка невідомих слів\n",
        "                word = assign_unk(word)\n",
        "            return word, tag\n",
        "        else:\n",
        "            # Обробка некоректних рядків\n",
        "            return \"--n--\", \"--s--\"  # Виправлення: додано повернення для некоректних рядків\n",
        "\n",
        "\n",
        "def create_dictionaries(training_corpus, vocab):\n",
        "    \"\"\"\n",
        "    Створення словників частот.\n",
        "    \"\"\"\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "\n",
        "    # Початковий тег\n",
        "    prev_tag = '--s--'\n",
        "\n",
        "    for word_tag in training_corpus:\n",
        "        word, tag = get_word_tag(word_tag, vocab)\n",
        "\n",
        "        # Збільшення лічильника переходів\n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "\n",
        "        # Збільшення лічильника емісій\n",
        "        emission_counts[(tag, word)] += 1\n",
        "\n",
        "        # Збільшення лічильника тегів\n",
        "        tag_counts[tag] += 1\n",
        "\n",
        "        # Оновлення попереднього тегу\n",
        "        prev_tag = tag\n",
        "\n",
        "    return emission_counts, transition_counts, tag_counts\n"
      ],
      "metadata": {
        "id": "QzUZMtN8PWOM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    \"\"\"\n",
        "    Створення матриці переходів A.\n",
        "    \"\"\"\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(all_tags)\n",
        "\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        for j in range(num_tags):\n",
        "            key = (all_tags[i], all_tags[j])\n",
        "\n",
        "            count = 0\n",
        "            if key in transition_counts:\n",
        "                count = transition_counts[key]\n",
        "\n",
        "            count_prev_tag = tag_counts[all_tags[i]]\n",
        "\n",
        "            # Застосування згладжування\n",
        "            A[i, j] = (count + alpha) / (count_prev_tag + alpha * num_tags)\n",
        "\n",
        "    return A"
      ],
      "metadata": {
        "id": "NL06UbQTQQcD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
        "    \"\"\"\n",
        "    Створення матриці емісій B.\n",
        "    \"\"\"\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(tag_counts)\n",
        "    num_words = len(vocab)\n",
        "\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        for j in range(num_words):\n",
        "            key = (all_tags[i], vocab[j])\n",
        "\n",
        "            count = 0\n",
        "            if key in emission_counts:\n",
        "                count = emission_counts[key]\n",
        "\n",
        "            count_tag = tag_counts[all_tags[i]]\n",
        "\n",
        "            # Застосування згладжування\n",
        "            B[i, j] = (count + alpha) / (count_tag + alpha * num_words)\n",
        "\n",
        "    return B\n"
      ],
      "metadata": {
        "id": "-JZVwt90Q7z8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Виправлення 3: Оновлення функцій Вітербі для роботи з невідомими словами\n",
        "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
        "    \"\"\"\n",
        "    Ініціалізація алгоритму Вітербі.\n",
        "    \"\"\"\n",
        "    num_tags = len(tag_counts)\n",
        "\n",
        "    # Ініціалізація матриць best_probs та best_paths\n",
        "    best_probs = np.zeros((num_tags, len(corpus)))\n",
        "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
        "\n",
        "    # Індекс початкового стану\n",
        "    s_idx = states.index(\"--s--\")\n",
        "\n",
        "    # Заповнення першого стовпця best_probs\n",
        "    word = corpus[0]\n",
        "    word_idx = vocab.get(word, vocab.get(\"--unk--\", 0))  # Виправлення: обробка невідомих слів\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        best_probs[i, 0] = math.log(A[s_idx, i]) + math.log(B[i, word_idx])\n",
        "\n",
        "    return best_probs, best_paths\n",
        "\n",
        "def viterbi_forward(A, B, corpus, best_probs, best_paths, vocab):\n",
        "    \"\"\"\n",
        "    Пряме проходження алгоритму Вітербі.\n",
        "    \"\"\"\n",
        "    num_tags = best_probs.shape[0]\n",
        "\n",
        "    for i in range(1, len(corpus)):\n",
        "        word = corpus[i]\n",
        "        word_idx = vocab.get(word, vocab.get(\"--unk--\", 0))  # Виправлення: обробка невідомих слів\n",
        "\n",
        "        for j in range(num_tags):\n",
        "            best_prob_i = float(\"-inf\")\n",
        "            best_path_i = None\n",
        "\n",
        "            for k in range(num_tags):\n",
        "                prob = best_probs[k, i-1] + math.log(A[k, j]) + math.log(B[j, word_idx])\n",
        "\n",
        "                if prob > best_prob_i:\n",
        "                    best_prob_i = prob\n",
        "                    best_path_i = k\n",
        "\n",
        "            best_probs[j, i] = best_prob_i\n",
        "            best_paths[j, i] = best_path_i\n",
        "\n",
        "    return best_probs, best_paths\n",
        "\n",
        "\n",
        "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
        "    \"\"\"\n",
        "    Зворотне проходження алгоритму Вітербі.\n",
        "    \"\"\"\n",
        "    m = best_paths.shape[1]\n",
        "    z = [None] * m\n",
        "    pred = [None] * m\n",
        "\n",
        "    # Знаходження найкращого тегу для останнього слова\n",
        "    best_prob_for_last_word = float('-inf')\n",
        "\n",
        "    for k in range(best_probs.shape[0]):\n",
        "        if best_probs[k, m-1] > best_prob_for_last_word:\n",
        "            best_prob_for_last_word = best_probs[k, m-1]\n",
        "            z[m-1] = k\n",
        "\n",
        "    pred[m-1] = states[z[m-1]]\n",
        "\n",
        "    # Зворотне проходження для знаходження найкращих тегів\n",
        "    for i in range(m-2, -1, -1):\n",
        "        z[i] = best_paths[z[i+1], i+1]\n",
        "        pred[i] = states[z[i]]\n",
        "\n",
        "    return pred\n"
      ],
      "metadata": {
        "id": "uqvbWD3nRI7h"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(pred, y):\n",
        "    \"\"\"\n",
        "    Обчислення точності моделі POS-тегування.\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for prediction, y_item in zip(pred, y):\n",
        "        y_item = y_item.strip()\n",
        "        word_tag_tuple = y_item.split('\\t')\n",
        "\n",
        "        if len(word_tag_tuple) != 2:\n",
        "            continue\n",
        "\n",
        "        word, tag = word_tag_tuple\n",
        "\n",
        "        if tag == prediction:\n",
        "            num_correct += 1\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    return num_correct / total\n"
      ],
      "metadata": {
        "id": "iTFgGcrXRTyP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Виправлення 4: Код для запуску моделі\n",
        "# Створення словників\n",
        "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n",
        "\n",
        "# Отримання списку всіх тегів\n",
        "states = sorted(tag_counts.keys())\n"
      ],
      "metadata": {
        "id": "01OU98ScRheM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметр згладжування\n",
        "alpha = 0.001\n",
        "\n",
        "# Створення індексованого словника слів\n",
        "word_to_index = {}\n",
        "for i, word in enumerate(sorted(vocab.keys())):\n",
        "    word_to_index[word] = i\n",
        "\n",
        "# Додавання спеціальних токенів\n",
        "for special_token in [\"--n--\", \"--unk--\", \"--unk_digit--\", \"--unk_punct--\", \"--unk_upper--\",\n",
        "                      \"--unk_noun--\", \"--unk_verb--\", \"--unk_adj--\", \"--unk_adv--\"]:\n",
        "    if special_token not in word_to_index:\n",
        "        word_to_index[special_token] = len(word_to_index)\n"
      ],
      "metadata": {
        "id": "EnJMgVIqRofQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Створення матриць переходів та емісій\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(word_to_index.keys()))"
      ],
      "metadata": {
        "id": "egDe_PECRsAA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ініціалізація алгоритму Вітербі\n",
        "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, word_to_index)\n"
      ],
      "metadata": {
        "id": "QtnvO5TXSgx1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пряме проходження\n",
        "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, word_to_index)\n"
      ],
      "metadata": {
        "id": "Pmp_qETmSh-W"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Зворотне проходження для отримання найкращої послідовності тегів\n",
        "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
        "\n",
        "# Обчислення точності\n",
        "accuracy = compute_accuracy(pred, y)\n",
        "print(f\"Точність моделі: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFzFyTTDTAti",
        "outputId": "528465ed-7c8f-4375-d827-e8abb12ebdde"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точність моделі: 0.9580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "# Завантаження необхідних ресурсів\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('conll2000')\n",
        "\n",
        "# Приклад тексту\n",
        "text = \"The market's pulse will be checked through multiple diagnostic tools this quarter.\"\n",
        "\n",
        "# Токенізація тексту\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# POS-тегування з використанням NLTK\n",
        "nltk_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"Результат POS-тегування NLTK:\")\n",
        "for word, tag in nltk_tags:\n",
        "    print(f\"{word}: {tag}\")\n",
        "\n",
        "# Оцінка точності NLTK POS-tagger на корпусі treebank\n",
        "train_data = conll2000.tagged_sents()[:3000]\n",
        "test_data = conll2000.tagged_sents()[3000:]\n",
        "\n",
        "# Навчання NLTK тегера\n",
        "from nltk.tag import UnigramTagger, BigramTagger\n",
        "unigram_tagger = UnigramTagger(train_data)\n",
        "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
        "\n",
        "# Оцінка точності\n",
        "nltk_accuracy = bigram_tagger.evaluate(test_data)\n",
        "print(f\"Точність NLTK BigramTagger: {nltk_accuracy:.4f}\")\n",
        "\n",
        "# Порівняння з нашою реалізацією\n",
        "print(f\"Точність нашої реалізації: {accuracy:.4f}\")\n",
        "print(f\"Різниця: {abs(accuracy - nltk_accuracy):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXoFTYz5TKe6",
        "outputId": "8d27c04c-57ad-47a6-db38-c3b64721276b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат POS-тегування NLTK:\n",
            "The: DT\n",
            "market: NN\n",
            "'s: POS\n",
            "pulse: NN\n",
            "will: MD\n",
            "be: VB\n",
            "checked: VBN\n",
            "through: IN\n",
            "multiple: JJ\n",
            "diagnostic: JJ\n",
            "tools: NNS\n",
            "this: DT\n",
            "quarter: NN\n",
            ".: .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-96f66dc204d6>:34: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  nltk_accuracy = bigram_tagger.evaluate(test_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точність NLTK BigramTagger: 0.8590\n",
            "Точність нашої реалізації: 0.9580\n",
            "Різниця: 0.0989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7DLfadyJT-Qy"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}